{
  "hash": "a4b7213d46016cdcadd175df0acb3748",
  "result": {
    "markdown": "---\ntitle: \"Applying literate programming techniques in the workplace\"\ndescription: \"_Calum discusses the power of literate programming to communicate with clincal collaborators_\"\nauthor:\n  - name: Calum Nicholson\ndate: 2023-09-11\ncategories: [Health Data Science, Literate Programming] # self-defined categories\nimage: \"Calum-Nicholson.jpeg\"\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\n<div style=\"display:grid; grid-template-columns: 1fr 1fr;\">\n\n<div>\n\n::: author-meta-content\n{{< fa envelope title=\"An envelope\" >}} <a href=\"mailto:Calum.Nicholson@hri.org.au\">Calum.Nicholson@hri.org.au</a>\n:::\n\n::: author-meta-content\n{{< fa user title=\"A person\" >}} <a href=\"https://www.hri.org.au/our-research/clinical-research/mr-calum-nicholson\">hri.org.au/our-research/clinical-research/mr-calum-nicholson</a>\n:::\n\n::: author-meta-content\n{{< fa user title=\"A person\" >}} <a href=\"https://au.linkedin.com/in/calum-nicholson-13b256171\">linkedin.com/in/calum-nicholson-13b256171</a>\n:::\n\n</div>\n\n\n</div>\n\n::: aside\n::: {.round-border}\n![](Calum-Nicholson.jpeg)\n:::\n\nCalum has been working with the Heart Research Institute since 2018 and he is the National Coordinator for an Australia-wide congenital heart disease (CHD) registry. \n \nCalum completed the MSc Health Data Science in 2021. His dissertation used geographical information and analysis techniques to assist with service delivery planning for adults with Congenital Heart Disease an was published in [PLOS Digital Health](https://doi.org/10.1371/journal.pdig.0000253).\n:::\n\n### The need for literate programming\n\nWorking as data scientists, our analyses are useless if they cannot be communicated with our collaborators and audiences. Especially when working with topic area specialists such as clinicians, public health experts or clinical researchers, presenting the results of our data analysis is essential. Literate programming techniques are tools that allow for the combination of conventional word processing with code for data analysis. These tools have fast become one of the most useful tools that I was taught during my time completing the Master of Health Data Science with the Centre for Big Data Research in Health. Of course, you still need the key skills of a data scientist---data management, statistics, and visualisation---otherwise there would be nothing to include in a report! However, working with people who do not have the access to the analytic software that you might use means that we need to be able to take our analyses out of these systems so that other people can engage with our work.\n\nMarkdown is a common form of implementing a literature programming technique. It is a mark-up language that is used to format text documents. Unlike Microsoft Word, which is a [WYSIWYG](https://en.wikipedia.org/wiki/WYSIWYG) (/ˈwɪziwɪɡ/ WIZ-ee-wig, i.e. what you see is what you get!) editor, where changes are to text formatting are seen immediately, Markdown uses specific syntax to create formatted text. This does require learning some of the markdown language, but you can mostly get by with knowing a few basics such as headings, lists, and bold and italic text. There are many different tools that combine markdown language with code, [Jupyter Notebook](https://jupyter.org/try-jupyter/retro/notebooks/?path=notebooks/Intro.ipynb ) is a common one that works with many different coding languages. My experience is mostly with [R Markdown from R Studio](https://rmarkdown.rstudio.com/), which is an excellent tool for literate programming.\n\nI work for the [Heart Research Institute](https://www.hri.org.au/), coordinating a project that is developing a Bi-National Registry for Congenital Heart Disease in Australia and New Zealand. In this role we have moved through data collection, cleaning, and analysis phases. In all these areas, using literate programming tools to generate reports have become a key part of my workflows. I am often collaborating with clinical staff, often cardiologists, nurses, and neuropsychologists, to produce various outputs and these outputs are always a HTML report using R Markdown.\n\n\n### Data Collection Reports: did I receive that data that I think I did?\n\nData collection for our registry involves receiving extracts for clinical data sources, often with hundreds of thousands of records. It is important to make sure that the data that we receive is what was intended to be sent. To aid these processes, I have developed data collection reports using R Markdown. These simply overview everything that was received, with some simple exploratory data analysis. These reports often go back to data managers and clinicians and provide a very useful sanity check that can identify simple mistakes early in the process.\n\n::: {.panel-tabset} \n\n## Output\n\n![**Generating consort diagrams with `ggconsort`** A flow diagram for patient selection into the ACHD registry. Toggle to the code tab to see the underlying code.](consort_flow.jpg)\n\n## Code\n\nFirst a \"Study Cohort\" is generated. `dplyr` verbs can be used to filter the cohort that will appear in the flowchart. Second, the boxes and arrows are drawn and plotted with `ggplot`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_cohorts <- el.sample %>%\n                  cohort_start(\"Assessed for eligibility\") %>%\n                  # Define cohorts using named expressions\n                  cohort_define(\n                    # Patient with missing diagnosis or procedure codes\n                    missing = .full %>% filter(!has_dx & !has_proc), \n                    # only patients with diagnosis or procedure considered\n                    considered = .full %>% filter(has_dx | has_proc),\n                    # patients without congential heart disease\n                    ineligibile = .full %>% filter(!el_final & (has_dx | has_proc)),\n                    # patients with congential Heart Disease\n                    eligibile = .full %>% filter(el_final)) %>%\n                  # Provide text table for cohorts\n                  cohort_label(\n                    missing = \"No Diagnosis or Procedure\",\n                    considered = \"Patients considered for eligibility\",\n                    ineligibile = \"Ineligibile Patients\",\n                    eligibile = \"Eligibile Patients\")\n\nstudy_consort <- study_cohorts %>% \n                  # Add the boxes\n                  consort_box_add(\n                    \"full\", 0, 50, cohort_count_adorn(study_cohorts, .full)) %>%\n                  consort_box_add(\n                    \"considered\", 0, 30, cohort_count_adorn(study_cohorts, considered)) %>%\n                  consort_box_add(\n                    \"eligibile\", 0, 10, cohort_count_adorn(study_cohorts, eligibile)) %>%\n                  consort_box_add(\n                    \"missing\", 5, 40, cohort_count_adorn(study_cohorts, missing)) %>%\n                  consort_box_add(\n                    \"ineligibile\", 5, 20, cohort_count_adorn(study_cohorts, ineligibile)) %>%\n                  # Add the arrows\n                  consort_arrow_add(\n                    start = \"full\", start_side = \"bottom\",\n                    end = \"considered\", end_side = \"top\") %>%\n                  consort_arrow_add(\n                    start = \"considered\", start_side = \"bottom\",\n                    end = \"eligibile\", end_side = \"top\") %>%\n                  consort_arrow_add(\n                    end = \"missing\", end_side = \"left\", start_x = 0, start_y = 40) %>%\n                  consort_arrow_add(\n                    end = \"ineligibile\", end_side = \"left\", start_x = 0, start_y = 20) %>% \n              # Plot with ggplot\n              ggplot() +\n              geom_consort() +\n              theme_consort(margin_h = 15, margin_v = 1)\n\nstudy_consort\n```\n:::\n\n:::\n\n\n\nA flowchart is a very useful tool for communicating the data collection process and how an eligible population was determined. This has always been difficult to implement in R and has usually required some manual development in a Microsoft Office application. However, I have recently discovered the [`ggconsort`](https://higgi13425.github.io/ggconsort/) package that is great for making Consort-Style flowchart. This package can create simple consort diagrams that are perfect for demonstrating how a target population was reached. It uses very familiar dplyr and ggplot2 workflows that make it quite easy to pick up for those who are already familiar with the [`tidyverse`](https://www.tidyverse.org/).\n\n\n### Data Analysis: collaborating with topic area specialists.\n\nRecently, I have been working with neuropsychologists to analyse data in preparation for manuscripts. Unsurprisingly, I know very little about neuropsychology and the neuropsychologists have never used R before! Using literate programming as an iterative process to complete data analysis for a manuscript can really help bridge the gap between disciplines. Ideally, I would want to hand over my analyses with enough clarity that my collaborators can assess my methodologies from their expert perspectives and then write up their results and methods sections using only my reports. This requires not only providing the analysis but including some notes and comments about methodology used for cohort selection and statistical analysis. Since literate programming allows you to write prose alongside your code, it provides a great process for clearing writing out your methods right alongside the analysis and results. You can provide exactly what you think should be included in a manuscript to describe your analysis and your collaborators won’t have to try decipher your code to understand what you are doing.\n\n::: {.panel-tabset} \n\n## Output\n\n![**Creating Summary Tables with qrwraps2** Summary statistics for patients who had completed a DASS-21 survey (example from a recent poster). Toggle to the code tab to see the underlying code.](summary_table.png)\n\n## Code\n\nThe code below uses the `summary_table()` function from the `qwraps` package to create a nice summary table. The template is applied to the whole dataset, then stratified by disease complexity (using `dyplr::group_by()`). We can then add these two tables together with `cbind()` to create the final table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(qwraps_markup = \"markdown\")\n\n# Create Table Template for DASS-21 Summary\ndass_level_summary <-\n  list(\"Depression\" =\n         list(\"Normal\" = ~ n_perc0(dass_dep_level_clean == \"Normal\"),\n              \"Mild\" = ~ n_perc0(dass_dep_level_clean == \"Mild\"),\n              \"Moderate\" = ~ n_perc0(dass_dep_level_clean == \"Moderate\"),\n              \"Severe\" = ~ n_perc0(dass_dep_level_clean == \"Severe\"),\n              \"Extemely Severe\" = ~ n_perc0(dass_dep_level_clean == \"Extemely Severe\"),\n              ),\n       \"Anxiety\" =\n         list(\"Normal\" = ~ n_perc0(dass_anx_level_clean == \"Normal\"),\n              \"Mild\" = ~ n_perc0(dass_anx_level_clean == \"Mild\"),\n              \"Moderate\" = ~ n_perc0(dass_anx_level_clean == \"Moderate\"),\n              \"Severe\" = ~ n_perc0(dass_anx_level_clean == \"Severe\"),\n              \"Extemely Severe\" = ~ n_perc0(dass_anx_level_clean == \"Extemely Severe\"),\n              ),\n       \"Stress\" = \n         list(\"Normal\" = ~ n_perc0(dass_stress_level_clean == \"Normal\"),\n              \"Mild\" = ~ n_perc0(dass_stress_level_clean == \"Mild\"),\n              \"Moderate\" = ~ n_perc0(dass_stress_level_clean == \"Moderate\"),\n              \"Severe\" = ~ n_perc0(dass_stress_level_clean == \"Severe\"),\n              \"Extemely Severe\" = ~ n_perc0(dass_stress_level_clean == \"Extemely Severe\"),\n              )\n       )\n\n# Summary Table for whole dataset\ndass.level.whole <- summary_table(poster.population, dass_level_summary)\n\n# Summary table stratified by complexity\ndass.level.complexity <- summary_table(dyplr::group_by(poster.population, chd_complexity), dass_level_summary)\n\n# Combine both tables\ndass.level <- cbind(dass.level.whole, dass.level.complexity)\n\n# Print table with title\nprint(dass.level,\n      rtitle = \"DASS-21 Level - Summary Statistics\",\n      cnames( c(paste(\"All (N -\", poster.population %>% nrow, \")\", sep = \"\"),\n                paste(\"Mild (N -\", poster.population %>% filter(chd_complexity = \"Mild\") %>% nrow, \")\", sep = \"\"),\n                paste(\"Moderate (N -\", poster.population %>% filter(chd_complexity = \"Moderate\") %>% nrow, \")\", sep = \"\"),\n                paste(\"Severe (N -\", poster.population %>% filter(chd_complexity = \"Severe\") %>% nrow, \")\", sep = \"\")\n                )\n              )\n)\n```\n:::\n\n\n\n:::\n\n\n\nThese reports are also a great help to communicate the results of your analysis since we can include direct output from the analysis, figures, tables, and comments that help to interpret the results. Of these outputs, creating publication-ready tables has always been the most challenging to implement in the flow of an R Markdown report. There are packages like `knitr` and `kableExtra` that can format tables nicely, but I have found that creating the tables to go into this format can often be difficult and time consuming. I have recently found a great package called [`qwraps2`](https://github.com/dewittpe/qwraps2/) that has a lot of functions for reproducible reporting and `summary_table()` is probably my new favourite. It allows you to create a template table format that can be applied to your data. This allows for a very flexible and reproducible way to create summary tables of your data. It is especially useful for providing a demographic overview of your study population.\n\n\n\n### Where to start with literate programming?\n\nTo get started with literate programming in R check out the `RMarkdown` website at [rmarkdown.rstudio.com](https://rmarkdown.rstudio.com/index.html). You'll find step-by-step instructions to get started as well as a [gallery of examples](https://rmarkdown.rstudio.com/gallery.html) with heaps of inspiration! The `qwraps2` package has a detailed [vignette](https://cran.r-project.org/web/packages/qwraps2/vignettes/summary-statistics.html) showing how to build a variety of summary tables. Python users can check out [Jupyter Notebooks](https://jupyter.org/try-jupyter/retro/notebooks/?path=notebooks/Intro.ipynb ) from the Jupyter community. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}